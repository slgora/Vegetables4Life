# -----------------------------------------------------------------------------
# Install and load required packages
packages <- c("pbapply", "readxl", "dplyr", "httr", "jsonlite", "tidyverse")
installed <- packages %in% rownames(installed.packages())
if (any(!installed)) install.packages(packages[!installed])
lapply(packages, library, character.only = TRUE)

# -----------------------------------------------------------------------------
# Load processed plant list
#WCFP_plantlist_processed <- read_csv("Data_processed/WCFP_plantlist_processed_2025-08-04.csv")
WCFP_plantlist_CWR_processed <- read_csv("Agrobio_veg_list/CWR_genepools_for_veg_list_standardized_31_July_2025.csv")

# Load veg_list 25,000
WCFP_plantlist_processed <- read_excel("G:/.shortcut-targets-by-id/1kQBvFIumhKQHnxSXNhr7w70dvuHgheEO/GCCFP/Data and analyses/Processed_data/WCFP_plantlist_processed_2025-08-07.xlsx")


# -----------------------------------------------------------------------------
# Test run on first 10 taxa
subset_taxa <- WCFP_plantlist_processed$taxon_name_accepted[1:10]
gbif_data_test <- pbapply::pblapply(subset_taxa, get_gbif_data)

# Remove failed queries and combine
gbif_data_test_clean <- gbif_data_test[!sapply(gbif_data_test, is.null)]
gbif_data_test_all <- bind_rows(gbif_data_test_clean)

# View results and check success count
View(gbif_data_test_all)
cat("Successful taxa:", length(gbif_data_test_clean), "\n")

# -----------------------------------------------------------------------------
# Full run across all taxa
gbif_data_list <- pbapply::pblapply(WCFP_plantlist_processed$taxon_name_accepted, get_gbif_data)

# Filter out NULLs and combine
gbif_data_clean <- gbif_data_list[!sapply(gbif_data_list, is.null)]
gbif_data_all <- bind_rows(gbif_data_clean)

# Save results
write_csv(gbif_data_all, "Outputs/WCFP_CWR_gbif_occurrence_data.csv")












#### try 2:

library(readxl)
library(pbapply)
library(dplyr)
library(purrr)
library(readr)

# Load full species list
WCFP_plantlist_processed <- read_excel("G:/.shortcut-targets-by-id/1kQBvFIumhKQHnxSXNhr7w70dvuHgheEO/GCCFP/Data and analyses/Processed_data/WCFP_plantlist_processed_2025-08-07.xlsx")
species_list <- WCFP_plantlist_processed$taxon_name_accepted

# Define chunk size and split
chunk_size <- 500
chunks <- split(species_list, ceiling(seq_along(species_list) / chunk_size))

# Create output folder if needed
dir.create("Outputs/gbif_chunks", showWarnings = FALSE)

# Initialize failure log
failed_taxa <- list()

# Loop through chunks
for (i in seq_along(chunks)) {
  message(sprintf("Processing chunk %d of %d...", i, length(chunks)))
  chunk <- chunks[[i]]

  # Run GBIF queries with progress bar
  chunk_results <- pbapply::pblapply(chunk, function(name) {
    result <- get_gbif_data(name)
    if (is.null(result)) failed_taxa <<- c(failed_taxa, name)
    return(result)
  })

  # Save raw chunk results
  saveRDS(chunk_results, file = sprintf("Outputs/gbif_chunks/gbif_chunk_%03d.rds", i))

  # Save cleaned CSV for successful results
  chunk_clean <- chunk_results[!sapply(chunk_results, is.null)]
  if (length(chunk_clean) > 0) {
    chunk_df <- bind_rows(chunk_clean)
    write_csv(chunk_df, sprintf("Outputs/gbif_chunks/gbif_chunk_%03d_clean.csv", i))
  }
}

# Save failed taxa log
write_lines(failed_taxa, "Outputs/gbif_chunks/gbif_failed_taxa.txt")






#### Combine all cleaned CSV chunk files from the gbif_chunks folder
library(readr)
library(dplyr)
library(purrr)
library(data.table)

# Start timer
start_time <- Sys.time()

# List all cleaned CSV files
csv_files <- list.files("Outputs/gbif_chunks", pattern = "_clean\\.csv$", full.names = TRUE)
n_files <- length(csv_files)

# Split files into manageable batches
batch_size <- 10  # Adjust based on available RAM
file_batches <- split(csv_files, ceiling(seq_along(csv_files) / batch_size))
n_batches <- length(file_batches)

# Create temp directory for intermediate batch files
dir.create("Outputs/temp_batches", showWarnings = FALSE)

# Initialize batch progress bar
pb <- txtProgressBar(min = 0, max = n_batches, style = 3)

# Merge each batch and save to disk
batch_paths <- vector("character", length = n_batches)

for (i in seq_along(file_batches)) {
  message(paste("ðŸ”„ Starting batch", i, "of", n_batches))

  files <- file_batches[[i]]

  batch_df <- rbindlist(
    lapply(seq_along(files), function(j) {
      file <- files[j]
      message(paste("ðŸ“„ Reading file", j, "of", length(files), "in batch", i, ":", basename(file)))
      tryCatch(
        fread(file, showProgress = FALSE),
        error = function(e) {
          message(paste("âš ï¸ Error reading", basename(file), ":", e$message))
          NULL
        }
      )
    }),
    use.names = TRUE,
    fill = TRUE
  )

  batch_path <- paste0("Outputs/temp_batches/batch_", i, ".csv")
  fwrite(batch_df, batch_path)
  batch_paths[i] <- batch_path

  setTxtProgressBar(pb, i)
  rm(batch_df); gc()
}

close(pb)  # Close progress bar

# Final merge of all batch files
message("ðŸ”„ Final merge of all batch files...")
gbif_combined_df <- rbindlist(
  lapply(batch_paths, fread),
  use.names = TRUE,
  fill = TRUE
)

# Save the final combined dataset
# fwrite(gbif_combined_df, "Outputs/gbif_chunks/gbif_combined_clean.csv")
#save as RDS
saveRDS(gbif_combined_df, "gbif_combined_df.rds")
gbif_combined_df <- readRDS("gbif_combined_df.rds")

#save as csv
write.csv(gbif_combined_df, "gbif_combined_df2.csv", row.names = FALSE)

# End timer
end_time <- Sys.time()
duration <- end_time - start_time

# Print duration
message(paste("âœ… Chunk merge completed in", round(duration, 2), "seconds"))









file.remove("gbif_combined_df.rds")
rm(list = setdiff(ls(), "gbif_combined_df"))  # Keep only your big object
gc()  # Clean up memory


#save and compress
saveRDS(gbif_combined_df, "rgbif_combined_df.rds", compress = "gzip")



# maybe later: iucnRedListCategory




# filter out geo data
# only taxon name and geodata and basis of record
# Basis of Recordâ€ - delete fossil specimen

# lookup_name
# basisOfRecord - delete fossil
# decimalLatitude
# decimalLongitude
gbif_geo_data <- gbif_combined_df %>%
  select(lookup_name, basisOfRecord, decimalLatitude, decimalLongitude)




